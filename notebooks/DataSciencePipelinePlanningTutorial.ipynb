{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "juvenile-effectiveness",
   "metadata": {},
   "source": [
    "# Using AI planning to explore data science pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "invalid-amazon",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import sys\n",
    "import os\n",
    "import types\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), \"../grammar2lale\")))\n",
    "\n",
    "# Clean output directory where we store planning and result files\n",
    "os.system('rm -rf ../output')\n",
    "os.system('mkdir -p ../output')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "confidential-function",
   "metadata": {},
   "source": [
    "## 1. Start with a Data Science grammar, in EBNF format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "adjustable-breed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is the grammar file we will use\n",
    "GRAMMAR_FILE=\"../grammar/dsgrammar-subset-sklearn.bnf\"\n",
    "\n",
    "# Copy grammar to the output directory\n",
    "os.system(\"cp \" + GRAMMAR_FILE + \" ../output/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "norman-sewing",
   "metadata": {},
   "source": [
    "## 2. Convert the grammar into an HTN domain and problem and use [HTN to PDDL](https://github.com/ronwalf/HTN-Translation) to translate to a PDDL task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "raised-introduction",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating HTN specification from grammar\n",
      "Printing HTN domain\n"
     ]
    }
   ],
   "source": [
    "from grammar2lale import Grammar2Lale\n",
    "\n",
    "# Generate HTN specifications\n",
    "G2L = Grammar2Lale(grammar_file=GRAMMAR_FILE)\n",
    "with open(\"../output/domain.htn\", \"w\") as f:\n",
    "    f.write(G2L.htn_domain);\n",
    "with open(\"../output/problem.htn\", \"w\") as f:\n",
    "    f.write(G2L.htn_problem);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dominant-photography",
   "metadata": {},
   "source": [
    "## 3. Extend the PDDL task by integrating soft constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "rubber-network",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b418f4da5064ed0b6959409b21c3831",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=10, description='num_pipelines', min=1), SelectMultiple(description='Sea…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import re\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets\n",
    "\n",
    "# as a safety step, setting costs to 0 for any parts of the grammar that are non-identifiers (e.g., parens, etc.)\n",
    "for token in G2L.htn.mapping:\n",
    "    if not re.match('^[_a-zA-Z]', str(token)):\n",
    "        G2L.costs[token] = 0\n",
    "        \n",
    "# prepare the list of possible constraints\n",
    "constraint_options = G2L.get_selectable_constraints()\n",
    "constraint_options.sort()    \n",
    "\n",
    "# prepare a constraint selection form\n",
    "interact_pipeline_params=interact.options(manual=True, manual_name='Generate PDDL')\n",
    "\n",
    "\n",
    "pipelines = []\n",
    "NUM_PIPELINES = 10\n",
    "CONSTRAINTS = []\n",
    "\n",
    "\n",
    "# This is the function that handles the constraint selection\n",
    "@interact_pipeline_params(num_pipelines=widgets.IntSlider(value=10, min=1, max=100), \n",
    "                          constraints=widgets.SelectMultiple(options=constraint_options,\n",
    "                                           description='Search constraints',\n",
    "                                           rows=min(20, len(constraint_options))))\n",
    "def select_pipeline_gen_params(num_pipelines, constraints):\n",
    "    global pipelines\n",
    "    global NUM_PIPELINES\n",
    "    global CONSTRAINTS\n",
    "    NUM_PIPELINES = num_pipelines\n",
    "    CONSTRAINTS = list(constraints)\n",
    "    G2L.create_pddl_task(NUM_PIPELINES, CONSTRAINTS)\n",
    "    with open(\"../output/domain.pddl\", \"w\") as f:\n",
    "        f.write(G2L.last_task['domain'])\n",
    "    with open(\"../output/problem.pddl\", \"w\") as f:\n",
    "        f.write(G2L.last_task['problem'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "corrected-dominant",
   "metadata": {},
   "source": [
    "## 4. Use a planner to solve the planning task (in this case, [K*](https://github.com/ctpelok77/kstar) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bored-cooking",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running the planner...\n",
      "Created domain file in /tmp/81976c59-8cf6-4ba8-94a9-73bd33ac997a/domain.pddl\n",
      "Created problem file in /tmp/81976c59-8cf6-4ba8-94a9-73bd33ac997a/problem.pddl\n",
      "Running kstar /tmp/81976c59-8cf6-4ba8-94a9-73bd33ac997a/domain.pddl /tmp/81976c59-8cf6-4ba8-94a9-73bd33ac997a/problem.pddl --search \"kstar(blind(),k=50,json_file_to_dump=result.json)\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sh: 1: Syntax error: Bad fd number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO     Running translator.\n",
      "INFO     translator input: ['/tmp/81976c59-8cf6-4ba8-94a9-73bd33ac997a/domain.pddl', '/tmp/81976c59-8cf6-4ba8-94a9-73bd33ac997a/problem.pddl']\n",
      "INFO     translator arguments: []\n",
      "INFO     translator time limit: None\n",
      "INFO     translator memory limit: None\n",
      "INFO     callstring: /usr/bin/python3 /workspace/kstar/builds/release64/bin/translate/translate.py /tmp/81976c59-8cf6-4ba8-94a9-73bd33ac997a/domain.pddl /tmp/81976c59-8cf6-4ba8-94a9-73bd33ac997a/problem.pddl\n",
      "Parsing...\n",
      "Parsing: [0.000s CPU, 0.007s wall-clock]\n",
      "Normalizing task... [0.000s CPU, 0.001s wall-clock]\n",
      "Instantiating...\n",
      "Generating Datalog program... [0.000s CPU, 0.001s wall-clock]\n",
      "Normalizing Datalog program...\n",
      "Normalizing Datalog program: [0.020s CPU, 0.014s wall-clock]\n",
      "Preparing model... [0.000s CPU, 0.004s wall-clock]\n",
      "Generated 318 rules.\n",
      "Computing model... [0.030s CPU, 0.025s wall-clock]\n",
      "507 relevant atoms\n",
      "1714 auxiliary atoms\n",
      "2221 final queue length\n",
      "2426 total queue pushes\n",
      "Completing instantiation... [0.000s CPU, 0.006s wall-clock]\n",
      "Instantiating: [0.050s CPU, 0.051s wall-clock]\n",
      "Computing fact groups...\n",
      "Finding invariants...\n",
      "96 initial candidates\n",
      "Finding invariants: [0.090s CPU, 0.089s wall-clock]\n",
      "Checking invariant weight... [0.000s CPU, 0.000s wall-clock]\n",
      "Instantiating groups... [0.000s CPU, 0.000s wall-clock]\n",
      "Collecting mutex groups... [0.000s CPU, 0.000s wall-clock]\n",
      "Choosing groups...\n",
      "177 uncovered facts\n",
      "Choosing groups: [0.000s CPU, 0.000s wall-clock]\n",
      "Building translation key... [0.000s CPU, 0.001s wall-clock]\n",
      "Computing fact groups: [0.090s CPU, 0.090s wall-clock]\n",
      "Building STRIPS to SAS dictionary... [0.000s CPU, 0.000s wall-clock]\n",
      "Building dictionary for full mutex groups... [0.000s CPU, 0.000s wall-clock]\n",
      "Building mutex information...\n",
      "Building mutex information: [0.010s CPU, 0.000s wall-clock]\n",
      "Translating task...\n",
      "Processing axioms...\n",
      "Simplifying axioms... [0.000s CPU, 0.000s wall-clock]\n",
      "Processing axioms: [0.000s CPU, 0.000s wall-clock]\n",
      "Translating task: [0.000s CPU, 0.007s wall-clock]\n",
      "218 effect conditions simplified\n",
      "0 implied preconditions added\n",
      "Detecting unreachable propositions...\n",
      "0 operators removed\n",
      "0 axioms removed\n",
      "1 propositions removed\n",
      "Detecting unreachable propositions: [0.010s CPU, 0.005s wall-clock]\n",
      "Reordering and filtering variables...\n",
      "160 of 179 variables necessary.\n",
      "0 of 3 mutex groups necessary.\n",
      "224 of 224 operators necessary.\n",
      "0 of 0 axiom rules necessary.\n",
      "Reordering and filtering variables: [0.000s CPU, 0.003s wall-clock]\n",
      "Translator variables: 160\n",
      "Translator derived variables: 0\n",
      "Translator facts: 339\n",
      "Translator goal facts: 1\n",
      "Translator mutex groups: 0\n",
      "Translator total mutex groups size: 0\n",
      "Translator operators: 224\n",
      "Translator axioms: 0\n",
      "Translator task size: 1697\n",
      "Translator peak memory: 31976 KB\n",
      "Writing output... [0.000s CPU, 0.003s wall-clock]\n",
      "Done! [0.160s CPU, 0.169s wall-clock]\n",
      "INFO     Running search (release64).\n",
      "INFO     search input: output.sas\n",
      "INFO     search arguments: ['--search', 'kstar(blind(),k=50,json_file_to_dump=result.json)']\n",
      "INFO     search time limit: None\n",
      "INFO     search memory limit: None\n",
      "INFO     search executable: /workspace/kstar/builds/release64/bin/downward\n",
      "INFO     callstring: /workspace/kstar/builds/release64/bin/downward --search 'kstar(blind(),k=50,json_file_to_dump=result.json)' --internal-plan-file sas_plan < output.sas\n",
      "reading input... [t=5.5546e-05s]\n",
      "done reading input! [t=0.00122401s]\n",
      "packing state variables...done! [t=0.00124533s]\n",
      "Variables: 161\n",
      "FactPairs: 341\n",
      "Bytes per state: 24\n",
      "Building successor generator...done! [t=0.00242805s]\n",
      "done initalizing global data [t=0.00244177s]\n",
      "Initializing blind search heuristic...\n",
      "Running K* with K=50\n",
      "Conducting best first search with reopening closed nodes, (real) bound = 2147483647\n",
      "New best heuristic value for blind: 0\n",
      "[g=0, 1 evaluated, 0 expanded, t=0.0025868s, 5436 KB]\n",
      "f = 0 [1 evaluated, 0 expanded, t=0.00263629s, 5436 KB]\n",
      "Initial heuristic value for blind: 0\n",
      "pruning method: none\n",
      "f = 1 [2 evaluated, 1 expanded, t=0.00267497s, 5436 KB]\n",
      "f = 2 [3 evaluated, 2 expanded, t=0.00270302s, 5436 KB]\n",
      "f = 3 [8 evaluated, 3 expanded, t=0.00273756s, 5436 KB]\n",
      "f = 4 [26 evaluated, 13 expanded, t=0.00284886s, 5436 KB]\n",
      "f = 5 [49 evaluated, 25 expanded, t=0.00297096s, 5436 KB]\n",
      "f = 6 [58 evaluated, 48 expanded, t=0.00312152s, 5436 KB]\n",
      "f = 53 [58 evaluated, 54 expanded, t=0.00318231s, 5436 KB]\n",
      "f = 54 [65 evaluated, 57 expanded, t=0.00322792s, 5436 KB]\n",
      "f = 55 [83 evaluated, 67 expanded, t=0.0033273s, 5436 KB]\n",
      "f = 56 [113 evaluated, 83 expanded, t=0.0034956s, 5436 KB]\n",
      "f = 57 [130 evaluated, 112 expanded, t=0.00368786s, 5436 KB]\n",
      "f = 58 [134 evaluated, 126 expanded, t=0.00380076s, 5436 KB]\n",
      "f = 104 [134 evaluated, 129 expanded, t=0.00383666s, 5436 KB]\n",
      "f = 105 [139 evaluated, 132 expanded, t=0.00387719s, 5436 KB]\n",
      "f = 106 [145 evaluated, 136 expanded, t=0.00393635s, 5436 KB]\n",
      "f = 107 [158 evaluated, 146 expanded, t=0.00403246s, 5436 KB]\n",
      "f = 108 [163 evaluated, 157 expanded, t=0.0041246s, 5436 KB]\n",
      "f = 155 [163 evaluated, 160 expanded, t=0.00416086s, 5436 KB]\n",
      "f = 156 [167 evaluated, 163 expanded, t=0.00421873s, 5436 KB]\n",
      "f = 157 [171 evaluated, 165 expanded, t=0.0042563s, 5436 KB]\n",
      "f = 158 [184 evaluated, 174 expanded, t=0.00434071s, 5436 KB]\n",
      "f = 159 [188 evaluated, 184 expanded, t=0.00441034s, 5436 KB]\n",
      "f = 208 [188 evaluated, 186 expanded, t=0.00443989s, 5436 KB]\n",
      "Completely explored state space!\n",
      "Actual search time: 0.00439784s [t=0.00706032s]\n",
      "Expanded 191 state(s).\n",
      "Reopened 0 state(s).\n",
      "Evaluated 191 state(s).\n",
      "Evaluations: 191\n",
      "Generated 262 state(s).\n",
      "Dead ends: 0 state(s).\n",
      "Expanded until last jump: 186 state(s).\n",
      "Reopened until last jump: 0 state(s).\n",
      "Evaluated until last jump: 188 state(s).\n",
      "Generated until last jump: 257 state(s).\n",
      "Number of plans found: 50\n",
      "Number of optimal plans found: 20\n",
      "Number of djkstra runs: 2\n",
      "Total number of djkstra node generations: 105\n",
      "Number of registered states: 191\n",
      "Search time: 0.00450275s\n",
      "Total time: 0.00707578s\n",
      "Enough solutions found.\n",
      "Peak memory: 5436 KB\n",
      "Plans returned after 1.1194465160369873 seconds.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "G2L.run_pddl_planner()\n",
    "with open(\"../output/first_planner_call.json\", \"w\") as f:\n",
    "    f.write(json.dumps(G2L.last_planner_object, indent=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "developmental-contemporary",
   "metadata": {},
   "source": [
    "## 5. Translate plans to [LALE](https://github.com/IBM/lale) Data Science pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "chinese-nashville",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translating plans to LALE pipelines.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "440b0552316749b1add7da4756872f5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='pipeline', options=(['Normalizer() >> LogisticRegression()', {'id'…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.show_pipeline(pipeline)>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Translate to pipelines\n",
    "pipelines = G2L.translate_to_pipelines(NUM_PIPELINES)\n",
    "\n",
    "from pipeline_optimizer import PipelineOptimizer\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "from lale.helpers import to_graphviz\n",
    "from lale.lib.sklearn import *\n",
    "from lale.lib.lale import ConcatFeatures as Concat\n",
    "from lale.lib.lale import NoOp\n",
    "from lale.lib.sklearn import KNeighborsClassifier as KNN\n",
    "from lale.lib.sklearn import OneHotEncoder as OneHotEnc\n",
    "from lale.lib.sklearn import Nystroem\n",
    "from lale.lib.sklearn import PCA\n",
    "\n",
    "optimizer = PipelineOptimizer(load_iris(return_X_y=True))\n",
    "# instantiate LALE objects from pipeline definitions\n",
    "LALE_pipelines = [optimizer.to_lale_pipeline(p) for p in pipelines]\n",
    "\n",
    "# Display selected pipeline\n",
    "def show_pipeline(pipeline):\n",
    "    print(\"Displaying pipeline \" + pipeline['id'] + \", with cost \" + str(pipeline['score']))\n",
    "    print(pipeline['pipeline'])\n",
    "    print('==================================================================================')\n",
    "    print()\n",
    "    print()\n",
    "    print()\n",
    "    display(to_graphviz(pipeline['lale_pipeline']))\n",
    "\n",
    "display_pipelines = [[p['pipeline'], p] for p in LALE_pipelines]    \n",
    "    \n",
    "interact(show_pipeline, pipeline=display_pipelines)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "absent-theory",
   "metadata": {},
   "source": [
    "## 6. Run one of the pipelines on sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "second-aluminum",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "italian-bundle",
   "metadata": {},
   "source": [
    "## 7. Train hyperparameters and evaluate the resulting LALE pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "suitable-welding",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plan 1/10\n",
      "Starting to optimize Normalizer() >> LogisticRegression()\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 20/20 [00:04<00:00,  4.87trial/s, best loss: -0.8099999999999999]\n",
      "Fit completed.\n",
      "Predict completed.\n",
      "Best accuracy: 0.8866666666666667\n",
      "Completed optimization for Normalizer() >> LogisticRegression()\n",
      "Plan 2/10\n",
      "Starting to optimize Normalizer() >> GaussianNB()\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 20/20 [00:04<00:00,  4.66trial/s, best loss: -0.97]\n",
      "Fit completed.\n",
      "Predict completed.\n",
      "Best accuracy: 0.9733333333333334\n",
      "Completed optimization for Normalizer() >> GaussianNB()\n",
      "Plan 3/10\n",
      "Starting to optimize Normalizer() >> DecisionTreeClassifier()\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 20/20 [00:07<00:00,  2.85trial/s, best loss: -0.9]\n",
      "Fit completed.\n",
      "Predict completed.\n",
      "Best accuracy: 0.94\n",
      "Completed optimization for Normalizer() >> DecisionTreeClassifier()\n",
      "Plan 4/10\n",
      "Starting to optimize Normalizer() >> QuadraticDiscriminantAnalysis()\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 20/20 [00:03<00:00,  5.66trial/s, best loss: -0.96]\n",
      "Fit completed.\n",
      "Predict completed.\n",
      "Best accuracy: 0.9733333333333334\n",
      "Completed optimization for Normalizer() >> QuadraticDiscriminantAnalysis()\n",
      "Plan 5/10\n",
      "Starting to optimize Normalizer() >> KNeighborsClassifier()\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 20/20 [00:06<00:00,  3.18trial/s, best loss: -0.97]\n",
      "Fit completed.\n",
      "Predict completed.\n",
      "Best accuracy: 1.0\n",
      "Completed optimization for Normalizer() >> KNeighborsClassifier()\n",
      "Plan 6/10\n",
      "Starting to optimize StandardScaler() >> LogisticRegression()\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 20/20 [00:05<00:00,  3.98trial/s, best loss: -0.97]\n",
      "Fit completed.\n",
      "Predict completed.\n",
      "Best accuracy: 0.98\n",
      "Completed optimization for StandardScaler() >> LogisticRegression()\n",
      "Plan 7/10\n",
      "Starting to optimize RobustScaler() >> LogisticRegression()\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 20/20 [00:04<00:00,  4.54trial/s, best loss: -0.97]\n",
      "Fit completed.\n",
      "Predict completed.\n",
      "Best accuracy: 0.98\n",
      "Completed optimization for RobustScaler() >> LogisticRegression()\n",
      "Plan 8/10\n",
      "Starting to optimize MinMaxScaler() >> LogisticRegression()\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 20/20 [00:04<00:00,  4.91trial/s, best loss: -0.89]\n",
      "Fit completed.\n",
      "Predict completed.\n",
      "Best accuracy: 0.94\n",
      "Completed optimization for MinMaxScaler() >> LogisticRegression()\n",
      "Plan 9/10\n",
      "Starting to optimize StandardScaler() >> KNeighborsClassifier()\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 20/20 [00:07<00:00,  2.84trial/s, best loss: -0.96]\n",
      "Fit completed.\n",
      "Predict completed.\n",
      "Best accuracy: 1.0\n",
      "Completed optimization for StandardScaler() >> KNeighborsClassifier()\n",
      "Plan 10/10\n",
      "Starting to optimize RobustScaler() >> KNeighborsClassifier()\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 20/20 [00:06<00:00,  3.16trial/s, best loss: -0.97]\n",
      "Fit completed.\n",
      "Predict completed.\n",
      "Best accuracy: 1.0\n",
      "Completed optimization for RobustScaler() >> KNeighborsClassifier()\n"
     ]
    }
   ],
   "source": [
    "trained_pipelines, dropped_pipelines = optimizer.evaluate_and_train_pipelines(pipelines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "japanese-spouse",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead>\n",
       "<tr><th>Pipeline                                                                                                                                                                                                                                                                                                                                                             </th><th style=\"text-align: right;\">  Accuracy</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>from lale.lib.lale import Hyperopt<br/>from sklearn.preprocessing import Normalizer<br/>from sklearn.linear_model import LogisticRegression<br/>import lale<br/><br/>lale.wrap_imported_operators()<br/>pipeline = Hyperopt(<br/>    estimator=Normalizer() >> LogisticRegression(), max_evals=20, scoring=\"r2\"<br/>)                                                </td><td style=\"text-align: right;\">  0.886667</td></tr>\n",
       "<tr><td>from lale.lib.lale import Hyperopt<br/>from sklearn.preprocessing import Normalizer<br/>from sklearn.naive_bayes import GaussianNB<br/>import lale<br/><br/>lale.wrap_imported_operators()<br/>pipeline = Hyperopt(<br/>    estimator=Normalizer() >> GaussianNB(), max_evals=20, scoring=\"r2\"<br/>)                                                                 </td><td style=\"text-align: right;\">  0.973333</td></tr>\n",
       "<tr><td>from lale.lib.lale import Hyperopt<br/>from sklearn.preprocessing import Normalizer<br/>from sklearn.tree import DecisionTreeClassifier<br/>import lale<br/><br/>lale.wrap_imported_operators()<br/>pipeline = Hyperopt(<br/>    estimator=Normalizer() >> DecisionTreeClassifier(),<br/>    max_evals=20,<br/>    scoring=\"r2\",<br/>)                               </td><td style=\"text-align: right;\">  0.94    </td></tr>\n",
       "<tr><td>from lale.lib.lale import Hyperopt<br/>from sklearn.preprocessing import Normalizer<br/>from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis<br/>import lale<br/><br/>lale.wrap_imported_operators()<br/>pipeline = Hyperopt(<br/>    estimator=Normalizer() >> QuadraticDiscriminantAnalysis(),<br/>    max_evals=20,<br/>    scoring=\"r2\",<br/>)</td><td style=\"text-align: right;\">  0.973333</td></tr>\n",
       "<tr><td>from lale.lib.lale import Hyperopt<br/>from sklearn.preprocessing import Normalizer<br/>from sklearn.neighbors import KNeighborsClassifier as KNN<br/>import lale<br/><br/>lale.wrap_imported_operators()<br/>pipeline = Hyperopt(<br/>    estimator=Normalizer() >> KNN(), max_evals=20, scoring=\"r2\"<br/>)                                                         </td><td style=\"text-align: right;\">  1       </td></tr>\n",
       "<tr><td>from lale.lib.lale import Hyperopt<br/>from sklearn.preprocessing import StandardScaler<br/>from sklearn.linear_model import LogisticRegression<br/>import lale<br/><br/>lale.wrap_imported_operators()<br/>pipeline = Hyperopt(<br/>    estimator=StandardScaler() >> LogisticRegression(),<br/>    max_evals=20,<br/>    scoring=\"r2\",<br/>)                       </td><td style=\"text-align: right;\">  0.98    </td></tr>\n",
       "<tr><td>from lale.lib.lale import Hyperopt<br/>from sklearn.preprocessing import RobustScaler<br/>from sklearn.linear_model import LogisticRegression<br/>import lale<br/><br/>lale.wrap_imported_operators()<br/>pipeline = Hyperopt(<br/>    estimator=RobustScaler() >> LogisticRegression(),<br/>    max_evals=20,<br/>    scoring=\"r2\",<br/>)                           </td><td style=\"text-align: right;\">  0.98    </td></tr>\n",
       "<tr><td>from lale.lib.lale import Hyperopt<br/>from sklearn.preprocessing import MinMaxScaler<br/>from sklearn.linear_model import LogisticRegression<br/>import lale<br/><br/>lale.wrap_imported_operators()<br/>pipeline = Hyperopt(<br/>    estimator=MinMaxScaler() >> LogisticRegression(),<br/>    max_evals=20,<br/>    scoring=\"r2\",<br/>)                           </td><td style=\"text-align: right;\">  0.94    </td></tr>\n",
       "<tr><td>from lale.lib.lale import Hyperopt<br/>from sklearn.preprocessing import StandardScaler<br/>from sklearn.neighbors import KNeighborsClassifier as KNN<br/>import lale<br/><br/>lale.wrap_imported_operators()<br/>pipeline = Hyperopt(<br/>    estimator=StandardScaler() >> KNN(), max_evals=20, scoring=\"r2\"<br/>)                                                 </td><td style=\"text-align: right;\">  1       </td></tr>\n",
       "<tr><td>from lale.lib.lale import Hyperopt<br/>from sklearn.preprocessing import RobustScaler<br/>from sklearn.neighbors import KNeighborsClassifier as KNN<br/>import lale<br/><br/>lale.wrap_imported_operators()<br/>pipeline = Hyperopt(<br/>    estimator=RobustScaler() >> KNN(), max_evals=20, scoring=\"r2\"<br/>)                                                     </td><td style=\"text-align: right;\">  1       </td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "from tabulate import tabulate\n",
    "from lale.pretty_print import to_string\n",
    "\n",
    "def show_pipeline_accuracy(tp):\n",
    "    pipeline_table = [[to_string(p['trained_pipeline']).replace('\\n', '<br/>'), str(p['best_accuracy'])] for p in tp]\n",
    "    display(HTML(tabulate(pipeline_table, headers=['Pipeline', 'Accuracy'], tablefmt='html')))\n",
    "\n",
    "\n",
    "show_pipeline_accuracy(trained_pipelines)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "authorized-journalism",
   "metadata": {},
   "source": [
    "## 8. Use pipeline accuracy to compute new PDDL action costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "joined-logan",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead>\n",
       "<tr><th>Pipeline element               </th><th style=\"text-align: right;\">  Computed cost</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>Normalizer()                   </td><td style=\"text-align: right;\">             70</td></tr>\n",
       "<tr><td>>>                             </td><td style=\"text-align: right;\">              0</td></tr>\n",
       "<tr><td>LogisticRegression()           </td><td style=\"text-align: right;\">             74</td></tr>\n",
       "<tr><td>GaussianNB()                   </td><td style=\"text-align: right;\">             61</td></tr>\n",
       "<tr><td>DecisionTreeClassifier()       </td><td style=\"text-align: right;\">             77</td></tr>\n",
       "<tr><td>QuadraticDiscriminantAnalysis()</td><td style=\"text-align: right;\">             61</td></tr>\n",
       "<tr><td>KNeighborsClassifier()         </td><td style=\"text-align: right;\">             50</td></tr>\n",
       "<tr><td>StandardScaler()               </td><td style=\"text-align: right;\">             54</td></tr>\n",
       "<tr><td>RobustScaler()                 </td><td style=\"text-align: right;\">             54</td></tr>\n",
       "<tr><td>MinMaxScaler()                 </td><td style=\"text-align: right;\">             77</td></tr>\n",
       "<tr><td>(                              </td><td style=\"text-align: right;\">              0</td></tr>\n",
       "<tr><td>)                              </td><td style=\"text-align: right;\">              0</td></tr>\n",
       "<tr><td>&                              </td><td style=\"text-align: right;\">              0</td></tr>\n",
       "<tr><td>                               </td><td style=\"text-align: right;\">              0</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "feedback = optimizer.get_feedback(trained_pipelines)\n",
    "G2L.feedback(feedback)\n",
    "costs_table = [[str(k), G2L.costs[k]] for k in G2L.costs.keys()]\n",
    "display(HTML(tabulate(costs_table, headers=['Pipeline element', 'Computed cost'], tablefmt='html')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stupid-logic",
   "metadata": {},
   "source": [
    "## 9. Invoke planner again on updated PDDL task and translate to pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "conceptual-roots",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating PDDL description...\n",
      "Obtaining 10 plans with constraints []\n",
      "Running the planner...\n",
      "Created domain file in /tmp/61e5fc61-b4f6-4948-90cb-ba0225c2f591/domain.pddl\n",
      "Created problem file in /tmp/61e5fc61-b4f6-4948-90cb-ba0225c2f591/problem.pddl\n",
      "Running kstar /tmp/61e5fc61-b4f6-4948-90cb-ba0225c2f591/domain.pddl /tmp/61e5fc61-b4f6-4948-90cb-ba0225c2f591/problem.pddl --search \"kstar(blind(),k=50,json_file_to_dump=result.json)\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sh: 1: Syntax error: Bad fd number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO     Running translator.\n",
      "INFO     translator input: ['/tmp/61e5fc61-b4f6-4948-90cb-ba0225c2f591/domain.pddl', '/tmp/61e5fc61-b4f6-4948-90cb-ba0225c2f591/problem.pddl']\n",
      "INFO     translator arguments: []\n",
      "INFO     translator time limit: None\n",
      "INFO     translator memory limit: None\n",
      "INFO     callstring: /usr/bin/python3 /workspace/kstar/builds/release64/bin/translate/translate.py /tmp/61e5fc61-b4f6-4948-90cb-ba0225c2f591/domain.pddl /tmp/61e5fc61-b4f6-4948-90cb-ba0225c2f591/problem.pddl\n",
      "Parsing...\n",
      "Parsing: [0.010s CPU, 0.007s wall-clock]\n",
      "Normalizing task... [0.000s CPU, 0.001s wall-clock]\n",
      "Instantiating...\n",
      "Generating Datalog program... [0.000s CPU, 0.001s wall-clock]\n",
      "Normalizing Datalog program...\n",
      "Normalizing Datalog program: [0.020s CPU, 0.013s wall-clock]\n",
      "Preparing model... [0.000s CPU, 0.004s wall-clock]\n",
      "Generated 318 rules.\n",
      "Computing model... [0.020s CPU, 0.024s wall-clock]\n",
      "507 relevant atoms\n",
      "1714 auxiliary atoms\n",
      "2221 final queue length\n",
      "2426 total queue pushes\n",
      "Completing instantiation... [0.020s CPU, 0.006s wall-clock]\n",
      "Instantiating: [0.060s CPU, 0.050s wall-clock]\n",
      "Computing fact groups...\n",
      "Finding invariants...\n",
      "96 initial candidates\n",
      "Finding invariants: [0.080s CPU, 0.086s wall-clock]\n",
      "Checking invariant weight... [0.000s CPU, 0.000s wall-clock]\n",
      "Instantiating groups... [0.000s CPU, 0.000s wall-clock]\n",
      "Collecting mutex groups... [0.000s CPU, 0.000s wall-clock]\n",
      "Choosing groups...\n",
      "177 uncovered facts\n",
      "Choosing groups: [0.000s CPU, 0.000s wall-clock]\n",
      "Building translation key... [0.010s CPU, 0.001s wall-clock]\n",
      "Computing fact groups: [0.090s CPU, 0.088s wall-clock]\n",
      "Building STRIPS to SAS dictionary... [0.000s CPU, 0.000s wall-clock]\n",
      "Building dictionary for full mutex groups... [0.000s CPU, 0.000s wall-clock]\n",
      "Building mutex information...\n",
      "Building mutex information: [0.000s CPU, 0.000s wall-clock]\n",
      "Translating task...\n",
      "Processing axioms...\n",
      "Simplifying axioms... [0.000s CPU, 0.000s wall-clock]\n",
      "Processing axioms: [0.000s CPU, 0.000s wall-clock]\n",
      "Translating task: [0.000s CPU, 0.007s wall-clock]\n",
      "218 effect conditions simplified\n",
      "0 implied preconditions added\n",
      "Detecting unreachable propositions...\n",
      "0 operators removed\n",
      "0 axioms removed\n",
      "1 propositions removed\n",
      "Detecting unreachable propositions: [0.000s CPU, 0.005s wall-clock]\n",
      "Reordering and filtering variables...\n",
      "160 of 179 variables necessary.\n",
      "0 of 3 mutex groups necessary.\n",
      "224 of 224 operators necessary.\n",
      "0 of 0 axiom rules necessary.\n",
      "Reordering and filtering variables: [0.010s CPU, 0.003s wall-clock]\n",
      "Translator variables: 160\n",
      "Translator derived variables: 0\n",
      "Translator facts: 339\n",
      "Translator goal facts: 1\n",
      "Translator mutex groups: 0\n",
      "Translator total mutex groups size: 0\n",
      "Translator operators: 224\n",
      "Translator axioms: 0\n",
      "Translator task size: 1697\n",
      "Translator peak memory: 31976 KB\n",
      "Writing output... [0.000s CPU, 0.003s wall-clock]\n",
      "Done! [0.170s CPU, 0.166s wall-clock]\n",
      "INFO     Running search (release64).\n",
      "INFO     search input: output.sas\n",
      "INFO     search arguments: ['--search', 'kstar(blind(),k=50,json_file_to_dump=result.json)']\n",
      "INFO     search time limit: None\n",
      "INFO     search memory limit: None\n",
      "INFO     search executable: /workspace/kstar/builds/release64/bin/downward\n",
      "INFO     callstring: /workspace/kstar/builds/release64/bin/downward --search 'kstar(blind(),k=50,json_file_to_dump=result.json)' --internal-plan-file sas_plan < output.sas\n",
      "reading input... [t=7.3387e-05s]\n",
      "done reading input! [t=0.0012769s]\n",
      "packing state variables...done! [t=0.00129551s]\n",
      "Variables: 161\n",
      "FactPairs: 341\n",
      "Bytes per state: 24\n",
      "Building successor generator...done! [t=0.00252175s]\n",
      "done initalizing global data [t=0.00253021s]\n",
      "Initializing blind search heuristic...\n",
      "Running K* with K=50\n",
      "Conducting best first search with reopening closed nodes, (real) bound = 2147483647\n",
      "New best heuristic value for blind: 0\n",
      "[g=0, 1 evaluated, 0 expanded, t=0.0026951s, 5436 KB]\n",
      "f = 0 [1 evaluated, 0 expanded, t=0.00275273s, 5436 KB]\n",
      "Initial heuristic value for blind: 0\n",
      "pruning method: none\n",
      "f = 1 [2 evaluated, 1 expanded, t=0.00279817s, 5436 KB]\n",
      "f = 2 [3 evaluated, 2 expanded, t=0.002827s, 5436 KB]\n",
      "f = 3 [8 evaluated, 3 expanded, t=0.00285926s, 5436 KB]\n",
      "f = 4 [26 evaluated, 13 expanded, t=0.00295215s, 5436 KB]\n",
      "f = 5 [49 evaluated, 25 expanded, t=0.00306095s, 5436 KB]\n",
      "f = 6 [58 evaluated, 48 expanded, t=0.00320101s, 5436 KB]\n",
      "f = 53 [58 evaluated, 54 expanded, t=0.00326068s, 5436 KB]\n",
      "f = 54 [65 evaluated, 57 expanded, t=0.00330282s, 5436 KB]\n",
      "f = 55 [83 evaluated, 67 expanded, t=0.00339557s, 5436 KB]\n",
      "f = 56 [106 evaluated, 80 expanded, t=0.00350957s, 5436 KB]\n",
      "f = 57 [122 evaluated, 106 expanded, t=0.0036733s, 5436 KB]\n",
      "f = 58 [130 evaluated, 118 expanded, t=0.00377062s, 5436 KB]\n",
      "f = 59 [134 evaluated, 126 expanded, t=0.00382847s, 5436 KB]\n",
      "f = 104 [134 evaluated, 129 expanded, t=0.00386257s, 5436 KB]\n",
      "f = 105 [139 evaluated, 132 expanded, t=0.00390072s, 5436 KB]\n",
      "f = 106 [145 evaluated, 136 expanded, t=0.00394946s, 5436 KB]\n",
      "f = 107 [155 evaluated, 141 expanded, t=0.00400711s, 5436 KB]\n",
      "f = 108 [162 evaluated, 156 expanded, t=0.00410246s, 5436 KB]\n",
      "f = 109 [163 evaluated, 159 expanded, t=0.00415683s, 5436 KB]\n",
      "f = 155 [163 evaluated, 160 expanded, t=0.00418354s, 5436 KB]\n",
      "f = 156 [167 evaluated, 163 expanded, t=0.0042202s, 5436 KB]\n",
      "f = 157 [171 evaluated, 165 expanded, t=0.00425275s, 5436 KB]\n",
      "f = 158 [181 evaluated, 169 expanded, t=0.00430512s, 5436 KB]\n",
      "f = 159 [188 evaluated, 184 expanded, t=0.00439977s, 5436 KB]\n",
      "f = 209 [188 evaluated, 186 expanded, t=0.00442983s, 5436 KB]\n",
      "Completely explored state space!\n",
      "Actual search time: 0.00439249s [t=0.00717411s]\n",
      "Expanded 191 state(s).\n",
      "Reopened 0 state(s).\n",
      "Evaluated 191 state(s).\n",
      "Evaluations: 191\n",
      "Generated 262 state(s).\n",
      "Dead ends: 0 state(s).\n",
      "Expanded until last jump: 186 state(s).\n",
      "Reopened until last jump: 0 state(s).\n",
      "Evaluated until last jump: 188 state(s).\n",
      "Generated until last jump: 257 state(s).\n",
      "Number of plans found: 50\n",
      "Number of optimal plans found: 1\n",
      "Number of djkstra runs: 2\n",
      "Total number of djkstra node generations: 105\n",
      "Number of registered states: 191\n",
      "Search time: 0.00451075s\n",
      "Total time: 0.00718761s\n",
      "Enough solutions found.\n",
      "Peak memory: 5436 KB\n",
      "Plans returned after 1.093231439590454 seconds.\n",
      "Translating plans to LALE pipelines.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead>\n",
       "<tr><th>First iteration                                </th><th>After feedback                                  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>Normalizer() >> LogisticRegression()           </td><td>PCA() >> KNeighborsClassifier()                 </td></tr>\n",
       "<tr><td>Normalizer() >> GaussianNB()                   </td><td>PCA() >> GradientBoostingClassifier()           </td></tr>\n",
       "<tr><td>Normalizer() >> DecisionTreeClassifier()       </td><td>PCA() >> RandomForestClassifier()               </td></tr>\n",
       "<tr><td>Normalizer() >> QuadraticDiscriminantAnalysis()</td><td>PCA() >> ExtraTreesClassifier()                 </td></tr>\n",
       "<tr><td>Normalizer() >> KNeighborsClassifier()         </td><td>StandardScaler() >> KNeighborsClassifier()      </td></tr>\n",
       "<tr><td>StandardScaler() >> LogisticRegression()       </td><td>RobustScaler() >> KNeighborsClassifier()        </td></tr>\n",
       "<tr><td>RobustScaler() >> LogisticRegression()         </td><td>StandardScaler() >> ExtraTreesClassifier()      </td></tr>\n",
       "<tr><td>MinMaxScaler() >> LogisticRegression()         </td><td>RobustScaler() >> ExtraTreesClassifier()        </td></tr>\n",
       "<tr><td>StandardScaler() >> KNeighborsClassifier()     </td><td>StandardScaler() >> GradientBoostingClassifier()</td></tr>\n",
       "<tr><td>RobustScaler() >> KNeighborsClassifier()       </td><td>RobustScaler() >> GradientBoostingClassifier()  </td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "new_pipelines = G2L.get_plans(num_pipelines=NUM_PIPELINES, constraints=CONSTRAINTS)\n",
    "\n",
    "with open('../output/domain_after_feedback.pddl', 'w') as f:\n",
    "    f.write(G2L.last_task['domain'])\n",
    "with open('../output/problem_after_feedback.pddl', 'w') as f:\n",
    "    f.write(G2L.last_task['problem'])\n",
    "with open('../output/second_planner_call.json', 'w') as f:\n",
    "    f.write(json.dumps(G2L.last_planner_object, indent=3))\n",
    "\n",
    "new_pipeline_table = [[pipelines[idx]['pipeline'], new_pipelines[idx]['pipeline']] for idx in range(min(len(pipelines), len(new_pipelines)))]\n",
    "display(HTML(tabulate(new_pipeline_table, headers=['First iteration', 'After feedback'], tablefmt='html')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced93591",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
