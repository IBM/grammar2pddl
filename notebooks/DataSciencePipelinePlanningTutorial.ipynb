{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "juvenile-effectiveness",
   "metadata": {},
   "source": [
    "# Using AI planning to explore data science pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "invalid-amazon",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import sys\n",
    "import os\n",
    "import types\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), \"../grammar2lale\")))\n",
    "\n",
    "# Clean output directory where we store planning and result files\n",
    "os.system('rm -rf ../output')\n",
    "os.system('mkdir -p ../output')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "confidential-function",
   "metadata": {},
   "source": [
    "## 1. Start with a Data Science grammar, in EBNF format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adjustable-breed",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# This is the grammar file we will use\n",
    "GRAMMAR_FILE=\"../grammar/dsgrammar-subset-sklearn.bnf\"\n",
    "\n",
    "# Copy grammar to the output directory\n",
    "os.system(\"cp \" + GRAMMAR_FILE + \" ../output/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa3d5c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat ../output/dsgrammar-subset-sklearn.bnf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "norman-sewing",
   "metadata": {},
   "source": [
    "## 2. Convert the grammar into an HTN domain and problem and use [HTN to PDDL](https://github.com/ronwalf/HTN-Translation) to translate to a PDDL task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "raised-introduction",
   "metadata": {},
   "outputs": [],
   "source": [
    "from grammar2lale import Grammar2Lale\n",
    "\n",
    "# Generate HTN specifications\n",
    "G2L = Grammar2Lale(grammar_file=GRAMMAR_FILE)\n",
    "with open(\"../output/domain.htn\", \"w\") as f:\n",
    "    f.write(G2L.htn_domain);\n",
    "with open(\"../output/problem.htn\", \"w\") as f:\n",
    "    f.write(G2L.htn_problem);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "808da740",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from grammarDiagram import sklearn_diagram\n",
    "with open('../output/grammar.svg', 'w') as f:\n",
    "    sklearn_diagram.writeSvg(f.write)\n",
    "from IPython.core.display import SVG\n",
    "SVG('../output/grammar.svg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941edc56",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat ../output/domain.htn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a938bf19",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat ../output/problem.htn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dominant-photography",
   "metadata": {},
   "source": [
    "## 3. Extend the PDDL task by integrating soft constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rubber-network",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets\n",
    "\n",
    "# as a safety step, setting costs to 0 for any parts of the grammar that are non-identifiers (e.g., parens, etc.)\n",
    "for token in G2L.htn.mapping:\n",
    "    if not re.match('^[_a-zA-Z]', str(token)):\n",
    "        G2L.costs[token] = 0\n",
    "        \n",
    "# prepare the list of possible constraints\n",
    "constraint_options = G2L.get_selectable_constraints()\n",
    "constraint_options.sort()    \n",
    "\n",
    "# prepare a constraint selection form\n",
    "interact_pipeline_params=interact.options(manual=True, manual_name='Generate PDDL')\n",
    "\n",
    "\n",
    "pipelines = []\n",
    "NUM_PIPELINES = 10\n",
    "CONSTRAINTS = []\n",
    "\n",
    "\n",
    "# This is the function that handles the constraint selection\n",
    "@interact_pipeline_params(num_pipelines=widgets.IntSlider(value=10, min=1, max=100), \n",
    "                          constraints=widgets.SelectMultiple(options=constraint_options,\n",
    "                                           description='Search constraints',\n",
    "                                           rows=min(20, len(constraint_options))))\n",
    "def select_pipeline_gen_params(num_pipelines, constraints):\n",
    "    global pipelines\n",
    "    global NUM_PIPELINES\n",
    "    global CONSTRAINTS\n",
    "    NUM_PIPELINES = num_pipelines\n",
    "    CONSTRAINTS = list(constraints)\n",
    "    G2L.create_pddl_task(NUM_PIPELINES, CONSTRAINTS)\n",
    "    with open(\"../output/domain.pddl\", \"w\") as f:\n",
    "        f.write(G2L.last_task['domain'])\n",
    "    with open(\"../output/problem.pddl\", \"w\") as f:\n",
    "        f.write(G2L.last_task['problem'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d22ae27",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat ../output/domain.pddl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de412288",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat ../output/problem.pddl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "corrected-dominant",
   "metadata": {},
   "source": [
    "## 4. Use a planner to solve the planning task (in this case, [K*](https://github.com/ctpelok77/kstar) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bored-cooking",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "G2L.run_pddl_planner()\n",
    "with open(\"../output/first_planner_call.json\", \"w\") as f:\n",
    "    f.write(json.dumps(G2L.last_planner_object, indent=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f5b73f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "!cat ../output/first_planner_call.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "developmental-contemporary",
   "metadata": {},
   "source": [
    "## 5. Translate plans to [LALE](https://github.com/IBM/lale) Data Science pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chinese-nashville",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Translate to pipelines\n",
    "pipelines = G2L.translate_to_pipelines(NUM_PIPELINES)\n",
    "\n",
    "from pipeline_optimizer import PipelineOptimizer\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "from lale.helpers import to_graphviz\n",
    "from lale.lib.sklearn import *\n",
    "from lale.lib.lale import ConcatFeatures as Concat\n",
    "from lale.lib.lale import NoOp\n",
    "from lale.lib.sklearn import KNeighborsClassifier as KNN\n",
    "from lale.lib.sklearn import OneHotEncoder as OneHotEnc\n",
    "from lale.lib.sklearn import Nystroem\n",
    "from lale.lib.sklearn import PCA\n",
    "\n",
    "optimizer = PipelineOptimizer(load_iris(return_X_y=True))\n",
    "# instantiate LALE objects from pipeline definitions\n",
    "LALE_pipelines = [optimizer.to_lale_pipeline(p) for p in pipelines]\n",
    "\n",
    "# Display selected pipeline\n",
    "def show_pipeline(pipeline):\n",
    "    print(\"Displaying pipeline \" + pipeline['id'] + \", with cost \" + str(pipeline['score']))\n",
    "    print(pipeline['pipeline'])\n",
    "    print('==================================================================================')\n",
    "    print()\n",
    "    print()\n",
    "    print()\n",
    "    display(to_graphviz(pipeline['lale_pipeline']))\n",
    "\n",
    "display_pipelines = [[p['pipeline'], p] for p in LALE_pipelines]    \n",
    "    \n",
    "interact(show_pipeline, pipeline=display_pipelines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d3239b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install 'liac-arff>=2.4.0'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "absent-theory",
   "metadata": {},
   "source": [
    "## 6. Optimize one of the pipelines on a small dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "second-aluminum",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lale.lib.lale import Hyperopt\n",
    "import lale.datasets.openml\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "PIPELINE_IDX = 0\n",
    "\n",
    "display(to_graphviz(LALE_pipelines[PIPELINE_IDX]['lale_pipeline']))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03cf8488",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = Hyperopt(\n",
    "    estimator=LALE_pipelines[PIPELINE_IDX]['lale_pipeline'],\n",
    "    max_evals=20,\n",
    "    scoring='accuracy'\n",
    ")\n",
    "X, y = load_iris(return_X_y=True)\n",
    "train_X, test_X, train_y, test_y = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=5489\n",
    ")\n",
    "\n",
    "pd.options.display.max_columns=None\n",
    "ds=pd.concat([pd.Series(y).head(5), pd.DataFrame(data=X).head(5)], axis=1)\n",
    "ds.columns=['species', 'sepal_length', 'sepal_width', 'petal_length', 'petal_width']\n",
    "display(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b747c1d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_pipeline = opt.fit(train_X, train_y)\n",
    "\n",
    "predictions = trained_pipeline.predict(test_X)\n",
    "best_accuracy = accuracy_score(test_y, [round(pred) for pred in predictions])\n",
    "print('Best accuracy: ' + str(best_accuracy))\n",
    "ds2 = pd.concat([pd.Series(predictions), pd.Series(test_y), pd.DataFrame(data=test_X)], axis=1)\n",
    "ds2.columns=['predicted species', 'species', 'sepal_length', 'sepal_width', 'petal_length', 'petal_width']\n",
    "display(ds2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "italian-bundle",
   "metadata": {},
   "source": [
    "## 7. Train hyperparameters and evaluate the resulting LALE pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suitable-welding",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trained_pipelines, dropped_pipelines = optimizer.evaluate_and_train_pipelines(pipelines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "japanese-spouse",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "from tabulate import tabulate\n",
    "from lale.pretty_print import to_string\n",
    "\n",
    "def show_pipeline_accuracy(tp):\n",
    "    pipeline_table = [[to_string(p['trained_pipeline']).replace('\\n', '<br/>'), str(p['best_accuracy'])] for p in tp]\n",
    "    display(HTML(tabulate(pipeline_table, headers=['Pipeline', 'Accuracy'], tablefmt='html')))\n",
    "\n",
    "\n",
    "show_pipeline_accuracy(trained_pipelines)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "authorized-journalism",
   "metadata": {},
   "source": [
    "## 8. Use pipeline accuracy to compute new PDDL action costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "joined-logan",
   "metadata": {},
   "outputs": [],
   "source": [
    "feedback = optimizer.get_feedback(trained_pipelines)\n",
    "G2L.feedback(feedback)\n",
    "costs_table = [[str(k), G2L.costs[k]] for k in G2L.costs.keys()]\n",
    "display(HTML(tabulate(costs_table, headers=['Pipeline element', 'Computed cost'], tablefmt='html')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stupid-logic",
   "metadata": {},
   "source": [
    "## 9. Invoke planner again on updated PDDL task and translate to pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conceptual-roots",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "new_pipelines = G2L.get_plans(num_pipelines=NUM_PIPELINES, constraints=CONSTRAINTS)\n",
    "\n",
    "with open('../output/domain_after_feedback.pddl', 'w') as f:\n",
    "    f.write(G2L.last_task['domain'])\n",
    "with open('../output/problem_after_feedback.pddl', 'w') as f:\n",
    "    f.write(G2L.last_task['problem'])\n",
    "with open('../output/second_planner_call.json', 'w') as f:\n",
    "    f.write(json.dumps(G2L.last_planner_object, indent=3))\n",
    "\n",
    "def build_and_show_new_table():\n",
    "    new_pipeline_table = [[pipelines[idx]['pipeline'], new_pipelines[idx]['pipeline']] for idx in range(min(len(pipelines), len(new_pipelines)))]\n",
    "    display(HTML(tabulate(new_pipeline_table, headers=['First iteration', 'After feedback'], tablefmt='html')))\n",
    "\n",
    "build_and_show_new_table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced93591",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat ../output/domain_after_feedback.pddl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d565e497",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat ../output/problem_after_feedback.pddl"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
